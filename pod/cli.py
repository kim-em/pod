"""pod — Multi-agent manager for Claude Code.

Manages concurrent autonomous Claude sessions with a TUI and CLI interface.
Each agent runs as an independent background process; the TUI is just a viewer.

Usage:
    pod                  # Interactive TUI
    pod init             # Bootstrap .pod/ in current git repo
    pod update           # Re-populate agent config from package
    pod add [N]          # Launch N new agents (default 1)
    pod list             # Show running agents
    pod finish [ID|all]  # Signal agent(s) to finish after current work
    pod kill [ID|all]    # Kill agent(s) immediately (unclaims issues)
    pod status           # Queue depth, agent count, total cost
    pod log [ID]         # Tail agent's session stdout
    pod config           # Print current config
    pod coordination ... # Run bundled coordination script
"""

from __future__ import annotations

import argparse
import contextlib
import curses
import dataclasses
import datetime
import fcntl
import json
import os
import random
import re
import signal
import importlib.resources
import shutil
import subprocess
import sys
import threading
import time
import tomllib
import uuid
from dataclasses import dataclass
from pathlib import Path

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------


def _find_project_dir() -> Path:
    """Walk up from cwd to find a directory containing .pod/."""
    d = Path.cwd().resolve()
    while True:
        if (d / ".pod").is_dir():
            return d
        if d.parent == d:
            break
        d = d.parent
    # Fallback: git root
    try:
        r = subprocess.run(["git", "rev-parse", "--show-toplevel"],
                           capture_output=True, text=True, timeout=5)
        if r.returncode == 0:
            return Path(r.stdout.strip())
    except Exception:
        pass
    return Path.cwd()


def _data_dir() -> Path:
    """Locate bundled data files from the installed package."""
    return Path(str(importlib.resources.files("pod.data")))


PROJECT_DIR = _find_project_dir()
POD_DIR = PROJECT_DIR / ".pod"
AGENTS_DIR = POD_DIR / "agents"
CONFIG_PATH = POD_DIR / "config.toml"
LOG_PATH = POD_DIR / "pod.log"
CLAIM_HISTORY_PATH = POD_DIR / "claim-history.json"
ISOLATED_CONFIG_DIR = POD_DIR / "claude-config"

# ---------------------------------------------------------------------------
# Default configuration (written on first run)
# ---------------------------------------------------------------------------

DEFAULT_CONFIG = """\
# pod configuration — all values have sensible defaults.
# Edit this file to customise agent behaviour.

[project]
worktree_base = "worktrees"        # Where git worktrees are created
session_dir = "sessions"           # Session stdout capture directory
build_cache_dir = ".lake"          # Build cache to rsync into worktrees
protected_files = ["PLAN.md"]      # Files agents may not modify in PRs

[claude]
model = "opus"                     # Claude model to use
quota_check = "~/.claude/skills/claude-usage/claude-available-model"
quota_retry_seconds = 60           # Sleep duration when quota unavailable
isolated_config = true             # Use isolated CLAUDE_CONFIG_DIR for agents

[pricing]
# Dollars per million tokens
input = 5.00
output = 25.00
cache_read = 0.50
cache_create = 6.25

[dispatch]
# Built-in strategies: "queue_balance", "round_robin"
# Or a path to a custom script (receives env vars, prints worker type name)
strategy = "queue_balance"
min_queue = 3                      # queue_balance: below this → planner-type

[monitor]
poll_interval = 2                  # Seconds between status updates
jsonl_stale_warning = 300          # Warn if JSONL unchanged for this many seconds
jsonl_missing_warning = 60         # Warn if JSONL not created after this many seconds
max_claim_restarts = 1             # Max times to auto-restart a dead session before releasing claim

# --- Worker Types ---
# Each [worker_types.<name>] defines a type of agent session.
# The dispatch strategy chooses among these.

[worker_types.plan]
prompt = "/plan"
lock = "planner"                   # Acquire this lock before running
copy_build_cache = false

[worker_types.work]
prompt = "/work"
copy_build_cache = true

# Example additional worker type:
# [worker_types.review]
# prompt = "/review"
# copy_build_cache = true
"""


def ensure_config() -> dict:
    """Load config, requiring pod init to have been run."""
    if not CONFIG_PATH.exists():
        print(f"No .pod/config.toml found. Run 'pod init' first.", file=sys.stderr)
        sys.exit(1)
    AGENTS_DIR.mkdir(parents=True, exist_ok=True)
    with open(CONFIG_PATH, "rb") as f:
        return tomllib.load(f)


def get_claude_config_dir(config: dict) -> Path | None:
    """Return isolated CLAUDE_CONFIG_DIR path, or None if disabled. No side effects."""
    if not cfg_get(config, "claude", "isolated_config", default=False):
        return None
    return ISOLATED_CONFIG_DIR


def ensure_claude_config(config: dict) -> Path | None:
    """Set up isolated CLAUDE_CONFIG_DIR for agents. Returns path or None if disabled."""
    config_dir = get_claude_config_dir(config)
    if config_dir is None:
        return None

    config_dir.mkdir(parents=True, exist_ok=True)

    # Minimal settings — no hooks, no plugins, no global CLAUDE.md
    settings_path = config_dir / "settings.json"
    settings_path.write_text('{"skipDangerousModePermissionPrompt": true}\n')

    # Symlink credentials from ~/.claude/ so subscription auth works.
    # Race-safe: use try/except since multiple agents may run this concurrently.
    real_claude = Path.home() / ".claude"
    cred_link = config_dir / ".credentials.json"
    cred_target = real_claude / ".credentials.json"
    if cred_target.exists():
        try:
            cred_link.unlink(missing_ok=True)
            cred_link.symlink_to(cred_target)
        except FileExistsError:
            pass  # Another process created it first — fine
    else:
        # Clean up stale symlink if source credential file is gone
        cred_link.unlink(missing_ok=True)

    # JSONL session storage
    (config_dir / "projects").mkdir(exist_ok=True)

    return config_dir


def _claude_projects_dir(claude_config_dir: Path | None = None) -> Path:
    """Return the directory containing JSONL project subdirs."""
    if claude_config_dir is not None:
        return claude_config_dir / "projects"
    return Path.home() / ".claude" / "projects"


def cfg_get(config: dict, *keys, default=None):
    """Nested dict lookup with default."""
    d = config
    for k in keys:
        if isinstance(d, dict):
            d = d.get(k, default)
        else:
            return default
    return d


# ---------------------------------------------------------------------------
# Logging
# ---------------------------------------------------------------------------

_log_lock = threading.Lock()


def log(msg: str):
    """Append timestamped message to pod.log."""
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}\n"
    with _log_lock:
        try:
            with open(LOG_PATH, "a") as f:
                f.write(line)
        except OSError:
            pass


def say(msg: str):
    """Print to stderr and log."""
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line, file=sys.stderr)
    log(msg)


# ---------------------------------------------------------------------------
# Claim history — persists which issues our local sessions have claimed
# ---------------------------------------------------------------------------

CLAIM_HISTORY_LOCK_PATH = POD_DIR / "claim-history.lock"


@contextlib.contextmanager
def _claim_history_filelock():
    """Cross-process exclusive lock for claim-history.json via fcntl.flock."""
    CLAIM_HISTORY_LOCK_PATH.touch()
    fd = open(CLAIM_HISTORY_LOCK_PATH)
    try:
        fcntl.flock(fd, fcntl.LOCK_EX)
        yield
    finally:
        fcntl.flock(fd, fcntl.LOCK_UN)
        fd.close()


def load_claim_history() -> dict:
    """Load {issue_num_str -> {session_uuid, short_id, restart_count}}."""
    try:
        return json.loads(CLAIM_HISTORY_PATH.read_text())
    except (OSError, json.JSONDecodeError):
        return {}


def _save_claim_history(history: dict):
    """Atomically write claim history."""
    tmp = CLAIM_HISTORY_PATH.with_suffix(".tmp")
    try:
        tmp.write_text(json.dumps(history, indent=2) + "\n")
        tmp.rename(CLAIM_HISTORY_PATH)
    except OSError as e:
        log(f"Failed to save claim history: {e}")


def record_claim(issue: int, session_uuid: str, short_id: str):
    """Record that our session claimed this issue. Preserves restart_count."""
    with _claim_history_filelock():
        history = load_claim_history()
        key = str(issue)
        history[key] = {
            "session_uuid": session_uuid,
            "short_id": short_id,
            "restart_count": history.get(key, {}).get("restart_count", 0),
        }
        _save_claim_history(history)


def clear_claim(issue: int):
    """Remove issue from claim history (called when PR is created)."""
    with _claim_history_filelock():
        history = load_claim_history()
        history.pop(str(issue), None)
        _save_claim_history(history)


# ---------------------------------------------------------------------------
# Data structures
# ---------------------------------------------------------------------------

@dataclass
class AgentState:
    """Mutable state for one agent, serialised to .pod/agents/<id>.json."""
    short_id: str = ""
    uuid: str = ""
    pid: int = 0
    pid_start_time: float = 0.0    # /proc start time — detects PID reuse
    worker_type: str = ""          # e.g. "work", "plan"
    status: str = "starting"       # starting, running, waiting_quota, finishing, stopped
    session_start: float = 0.0
    claimed_issue: int = 0
    pr_number: int = 0
    git_start: str = ""
    tokens_in: int = 0
    tokens_out: int = 0
    cache_read: int = 0
    cache_create: int = 0
    last_text: str = ""
    last_activity: float = 0.0
    finishing: bool = False
    force_quota: bool = False      # Skip quota check for this agent
    lock_held: str = ""            # Name of lock held (e.g. "planner"), or ""
    loop_iteration: int = 0
    worktree: str = ""
    branch: str = ""
    resume_session_uuid: str = ""  # If set, first iteration uses this UUID (to resume conversation)

    def cost(self, pricing: dict) -> float:
        """Calculate cost in dollars."""
        return (
            self.tokens_in * pricing.get("input", 5.0) / 1e6
            + self.cache_create * pricing.get("cache_create", 6.25) / 1e6
            + self.cache_read * pricing.get("cache_read", 0.50) / 1e6
            + self.tokens_out * pricing.get("output", 25.0) / 1e6
        )

    def to_dict(self) -> dict:
        return dataclasses.asdict(self)

    @classmethod
    def from_dict(cls, d: dict) -> AgentState:
        known = {f.name for f in dataclasses.fields(cls)}
        return cls(**{k: v for k, v in d.items() if k in known})

    def write(self):
        """Atomically write state to .pod/agents/<id>.json."""
        path = AGENTS_DIR / f"{self.short_id}.json"
        tmp = path.with_suffix(".tmp")
        try:
            tmp.write_text(json.dumps(self.to_dict(), indent=2) + "\n")
            tmp.rename(path)
        except OSError as e:
            log(f"Failed to write state for {self.short_id}: {e}")

    def remove_file(self):
        """Remove the state file."""
        path = AGENTS_DIR / f"{self.short_id}.json"
        try:
            path.unlink(missing_ok=True)
        except OSError:
            pass


def read_all_agents() -> list[AgentState]:
    """Read all agent state files, filtering out stale (dead process) ones."""
    agents = []
    if not AGENTS_DIR.exists():
        return agents
    for p in sorted(AGENTS_DIR.glob("*.json")):
        if p.suffix != ".json" or p.name.endswith(".tmp"):
            continue
        try:
            d = json.loads(p.read_text())
            agent = AgentState.from_dict(d)
            # Check if process is still alive (and not a reused PID)
            if agent.pid > 0:
                if not _pid_is_valid(agent.pid, agent.pid_start_time):
                    agent.status = "dead"
            agents.append(agent)
        except (json.JSONDecodeError, OSError):
            continue
    return agents


# ---------------------------------------------------------------------------
# Utilities
# ---------------------------------------------------------------------------

def _get_pid_start_time(pid: int) -> float:
    """Get process start time from /proc (Linux). Returns 0 if unavailable."""
    try:
        with open(f"/proc/{pid}/stat", "r") as f:
            # Field 22 (1-indexed) is starttime; split past last ')' for comm
            parts = f.read().split(")")[-1].split()
            return float(parts[19])  # starttime = field 22 - 2 (pid,comm) = index 19
    except (OSError, IndexError, ValueError):
        return 0.0


def _pid_is_valid(pid: int, expected_start: float) -> bool:
    """Check if PID is alive AND belongs to the expected process."""
    try:
        os.kill(pid, 0)
    except ProcessLookupError:
        return False
    except PermissionError:
        pass  # Exists but we can't signal it — still valid
    if expected_start > 0:
        actual = _get_pid_start_time(pid)
        if actual > 0 and actual != expected_start:
            return False  # PID reused by a different process
    return True


def human_size(n: int) -> str:
    if n >= 1048576:
        return f"{n / 1048576:.1f}MB"
    if n >= 1024:
        return f"{n / 1024:.1f}KB"
    return f"{n}B"


def human_duration(secs: int | float) -> str:
    secs = int(secs)
    if secs >= 3600:
        return f"{secs // 3600}h{secs % 3600 // 60:02d}m"
    if secs >= 60:
        return f"{secs // 60}m{secs % 60:02d}s"
    return f"{secs}s"


def timeago(iso_ts: str) -> str:
    """Convert ISO 8601 timestamp to relative time string like '2h ago'."""
    if not iso_ts:
        return ""
    try:
        dt = datetime.datetime.fromisoformat(iso_ts.replace("Z", "+00:00"))
        delta = datetime.datetime.now(datetime.timezone.utc) - dt
        secs = int(delta.total_seconds())
        if secs < 0:
            return ""
        if secs < 60:
            return f"{secs}s ago"
        if secs < 3600:
            return f"{secs // 60}m ago"
        if secs < 86400:
            return f"{secs // 3600}h ago"
        return f"{secs // 86400}d ago"
    except (ValueError, TypeError):
        return ""


def fmt_tokens(n: int) -> str:
    if n >= 1_000_000:
        return f"{n / 1_000_000:.1f}M"
    if n >= 1000:
        return f"{n / 1000:.0f}k"
    return str(n)


def token_summary(state: AgentState, pricing: dict) -> str:
    total_in = state.tokens_in + state.cache_read + state.cache_create
    if total_in == 0 and state.tokens_out == 0:
        return ""
    cost = state.cost(pricing)
    return f"{fmt_tokens(total_in)}/{fmt_tokens(state.tokens_out)}~${cost:.2f}"


def compute_historical_cost(pricing: dict,
                            claude_config_dir: Path | None = None) -> float:
    """Scan JSONL files for this project and return total estimated cost.

    Uses the same counting methodology as the JSONL monitor (sum all
    assistant records) for consistency with per-agent cost display.
    """
    # Compute the Claude projects-dir prefix for this project.
    # Claude Code munges paths: replace / with -, strip leading dots from components.
    def _claude_dir_name(p: Path) -> str:
        parts = str(p).split("/")
        cleaned = [part.lstrip(".") for part in parts]
        return "-".join(cleaned)

    project_prefix = _claude_dir_name(PROJECT_DIR)

    # Scan both ~/.claude/projects (historical) and isolated config (new sessions)
    projects_dirs = []
    real_projects = Path.home() / ".claude" / "projects"
    if real_projects.is_dir():
        projects_dirs.append(real_projects)
    if claude_config_dir is not None:
        iso_projects = claude_config_dir / "projects"
        if iso_projects.is_dir():
            projects_dirs.append(iso_projects)

    total_in = 0
    total_out = 0
    total_cache_read = 0
    total_cache_create = 0

    for projects_dir in projects_dirs:
        for d in projects_dir.iterdir():
            if not d.is_dir():
                continue
            name = d.name
            # Match this project's dirs (main + worktrees)
            if not (name == project_prefix or name.startswith(project_prefix + "-")):
                continue
            for f in d.glob("*.jsonl"):
                try:
                    with open(f) as fh:
                        for line in fh:
                            if '"usage"' not in line or '"assistant"' not in line:
                                continue
                            try:
                                rec = json.loads(line)
                            except json.JSONDecodeError:
                                continue
                            if rec.get("type") != "assistant":
                                continue
                            usage = rec.get("message", {}).get("usage", {})
                            total_in += usage.get("input_tokens", 0)
                            total_out += usage.get("output_tokens", 0)
                            total_cache_read += usage.get("cache_read_input_tokens", 0)
                            total_cache_create += usage.get("cache_creation_input_tokens", 0)
                except OSError:
                    continue

    return (
        total_in * pricing.get("input", 5.0) / 1e6
        + total_cache_create * pricing.get("cache_create", 6.25) / 1e6
        + total_cache_read * pricing.get("cache_read", 0.50) / 1e6
        + total_out * pricing.get("output", 25.0) / 1e6
    )


def check_quota(config: dict, force: bool = False) -> bool:
    """Check if Claude quota is available. Returns True if OK."""
    if force:
        return True
    cmd = os.path.expanduser(cfg_get(config, "claude", "quota_check", default=""))
    if not cmd:
        return True
    try:
        result = subprocess.run(
            [cmd], capture_output=True, text=True, timeout=30
        )
        if result.returncode != 0:
            return False
        model = result.stdout.strip()
        required = cfg_get(config, "claude", "model", default="opus")
        return model == required
    except (subprocess.TimeoutExpired, FileNotFoundError, OSError):
        return False


def coordination(config: dict, *args, env_extra: dict | None = None,
                 stdin_data: str | None = None) -> subprocess.CompletedProcess:
    """Run a coordination subcommand."""
    script = str(_data_dir() / "coordination")
    env = dict(os.environ)
    # Pass protected-files list so coordination can enforce it.
    pf = cfg_get(config, "project", "protected_files", default=["PLAN.md"])
    if isinstance(pf, list):
        pf = ":".join(pf)
    env["POD_PROTECTED_FILES"] = pf
    if env_extra:
        env.update(env_extra)
    return subprocess.run(
        [script, *args],
        capture_output=True, text=True, timeout=60,
        cwd=str(PROJECT_DIR), env=env,
        input=stdin_data,
    )


def get_queue_depth(config: dict) -> int:
    """Get number of unclaimed issues."""
    try:
        r = coordination(config, "queue-depth")
        return int(r.stdout.strip())
    except (ValueError, subprocess.TimeoutExpired):
        return 0


@dataclasses.dataclass
class GHItem:
    """An issue or PR for TUI display."""
    kind: str           # "issue" or "pr"
    number: int
    title: str
    labels: list[str]
    ci_status: str      # "" (unknown/none), "pass", "fail"
    state: str          # "open", "closed", "merged"
    timestamp: str      # ISO 8601 timestamp for the current state


def fetch_issues_and_prs() -> list[GHItem]:
    """Fetch issues (agent-plan label, all states) and recent PRs from GitHub."""
    items: list[GHItem] = []
    cwd = str(PROJECT_DIR)

    issue_json = "--json=number,title,labels,state,createdAt,updatedAt,closedAt"
    # All open issues (there should never be thousands of open ones)
    try:
        r = subprocess.run(
            ["gh", "issue", "list", "--label", "agent-plan", "--state", "open",
             "--limit", "500", issue_json],
            capture_output=True, text=True, timeout=30, cwd=cwd,
        )
        if r.returncode == 0:
            for iss in json.loads(r.stdout):
                labels = [l["name"] for l in iss.get("labels", [])]
                ts = iss.get("updatedAt") or iss.get("createdAt", "")
                items.append(GHItem(
                    kind="issue", number=iss["number"], title=iss["title"],
                    labels=labels, ci_status="", state="open", timestamp=ts,
                ))
    except (subprocess.TimeoutExpired, OSError, json.JSONDecodeError):
        pass
    # Recent closed issues (just enough for context; display logic drops these first anyway)
    try:
        r = subprocess.run(
            ["gh", "issue", "list", "--label", "agent-plan", "--state", "closed",
             "--limit", "30", issue_json],
            capture_output=True, text=True, timeout=30, cwd=cwd,
        )
        if r.returncode == 0:
            for iss in json.loads(r.stdout):
                labels = [l["name"] for l in iss.get("labels", [])]
                ts = iss.get("closedAt") or iss.get("updatedAt") or iss.get("createdAt", "")
                items.append(GHItem(
                    kind="issue", number=iss["number"], title=iss["title"],
                    labels=labels, ci_status="", state="closed", timestamp=ts,
                ))
    except (subprocess.TimeoutExpired, OSError, json.JSONDecodeError):
        pass

    # PRs (open + recently closed/merged)
    try:
        r = subprocess.run(
            ["gh", "pr", "list", "--state", "all", "--limit", "15",
             "--json", "number,title,labels,statusCheckRollup,state,createdAt,updatedAt,closedAt,mergedAt"],
            capture_output=True, text=True, timeout=30, cwd=cwd,
        )
        if r.returncode == 0:
            for pr in json.loads(r.stdout):
                labels = [l["name"] for l in pr.get("labels", [])]
                # CI status from statusCheckRollup
                ci = ""
                checks = pr.get("statusCheckRollup", []) or []
                if checks:
                    if any(c.get("conclusion") == "FAILURE" for c in checks):
                        ci = "fail"
                    elif (any(c.get("conclusion") == "SUCCESS" for c in checks) and
                          all(c.get("conclusion") == "SUCCESS" for c in checks if c.get("conclusion"))):
                        ci = "pass"
                pr_state = pr.get("state", "OPEN").lower()
                ts = pr.get("mergedAt") or pr.get("closedAt") or "" if pr_state in ("merged", "closed") else pr.get("updatedAt") or pr.get("createdAt", "")
                items.append(GHItem(
                    kind="pr", number=pr["number"], title=pr["title"],
                    labels=labels, ci_status=ci, state=pr_state, timestamp=ts,
                ))
    except (subprocess.TimeoutExpired, OSError, json.JSONDecodeError):
        pass

    # Sort by number descending (newest first), issues before PRs at same number
    items.sort(key=lambda x: (-x.number, x.kind))

    # Deduplicate: if an issue and PR share the same number, keep both
    # (they're different GitHub objects)
    return items


def fetch_blocked_deps() -> dict[int, list[int]]:
    """Fetch open depends-on dependencies for blocked issues (closed deps filtered out)."""
    import re as _re
    cwd = str(PROJECT_DIR)
    try:
        r = subprocess.run(
            ["gh", "issue", "list", "--label", "agent-plan", "--label", "blocked",
             "--state", "open", "--limit", "20", "--json", "number,body"],
            capture_output=True, text=True, timeout=30, cwd=cwd,
        )
        if r.returncode != 0:
            return {}

        raw: dict[int, list[int]] = {}
        for iss in json.loads(r.stdout):
            deps = [int(d) for d in _re.findall(r"depends-on: #(\d+)", iss.get("body", ""))]
            if deps:
                raw[iss["number"]] = deps
        if not raw:
            return {}

        # Fetch open issue numbers to filter out closed deps
        r2 = subprocess.run(
            ["gh", "issue", "list", "--state", "open", "--limit", "100",
             "--json", "number", "--jq", "[.[].number]"],
            capture_output=True, text=True, timeout=30, cwd=cwd,
        )
        open_nums: set[int] = set(json.loads(r2.stdout)) if r2.returncode == 0 else set()

        return {num: filtered for num, deps in raw.items()
                if (filtered := [d for d in deps if d in open_nums])}
    except (subprocess.TimeoutExpired, OSError, json.JSONDecodeError):
        return {}


# ---------------------------------------------------------------------------
# JSONL Monitor (runs as a thread inside agent process)
# ---------------------------------------------------------------------------

def jsonl_monitor(jsonl_path: str, state: AgentState, stop: threading.Event):
    """Poll JSONL file and update agent state. Runs in a daemon thread."""
    pos = 0
    while not stop.is_set():
        try:
            if not os.path.exists(jsonl_path):
                stop.wait(1)
                continue
            with open(jsonl_path, "rb") as f:
                f.seek(pos)
                while True:
                    line = f.readline()
                    if not line:
                        break
                    if not line.endswith(b"\n"):
                        break  # Partial line — retry next poll
                    pos += len(line)
                    _parse_jsonl_line(line, state)
        except OSError:
            pass
        stop.wait(1)


def _parse_jsonl_line(line: bytes, state: AgentState):
    """Parse one JSONL line and update state."""
    try:
        d = json.loads(line)
    except json.JSONDecodeError:
        return

    if d.get("type") != "assistant":
        return

    usage = d.get("message", {}).get("usage", {})
    state.tokens_in += usage.get("input_tokens", 0)
    state.cache_create += usage.get("cache_creation_input_tokens", 0)
    state.cache_read += usage.get("cache_read_input_tokens", 0)
    state.tokens_out += usage.get("output_tokens", 0)
    state.last_activity = time.time()

    for block in d.get("message", {}).get("content", []):
        btype = block.get("type")
        if btype == "text" and block.get("text", "").strip():
            state.last_text = block["text"].strip()
        elif btype == "tool_use":
            name = block.get("name", "")
            inp = block.get("input", {})
            detail = _tool_detail(name, inp, state)
            state.last_text = f"[{name}] {detail}" if detail else f"[{name}]"


def _tool_detail(name: str, inp: dict, state: AgentState) -> str:
    """Extract a display-friendly detail string from a tool invocation."""
    if name == "Bash":
        desc = inp.get("description", "")
        cmd = inp.get("command", "")
        # Detect coordination claim/create-pr
        m = re.search(r"(?:^|&&\s*|;\s*)(?:\./)?coordination\s+claim\s+(\d+)", cmd)
        if m:
            state.claimed_issue = int(m.group(1))
        m = re.search(r"(?:^|&&\s*|;\s*)(?:\./)?coordination\s+create-pr\s+(\d+)", cmd)
        if m:
            state.pr_number = int(m.group(1))
        if desc and cmd:
            return f"{desc}: {cmd}"
        return desc or cmd
    elif name == "Edit":
        p = inp.get("file_path", "")
        return p.split("/")[-1] if p else ""
    elif name in ("Read", "Write"):
        p = inp.get("file_path", "")
        return p.split("/")[-1] if p else ""
    elif name in ("Grep", "Glob"):
        return inp.get("pattern", "")
    elif name == "TodoWrite":
        todos = inp.get("todos", [])
        active = [t for t in todos if t.get("status") == "in_progress"]
        return active[0].get("activeForm", "") if active else ""
    elif name == "Task":
        return inp.get("description", "")
    return name


# ---------------------------------------------------------------------------
# Dispatch Strategies
# ---------------------------------------------------------------------------

def _choose_draining(config: dict, draining: dict) -> str | None:
    """Pick a random draining worker type that has available work.

    For types with issue_label, check per-label queue depth via
    `coordination queue-depth <label>`. Shuffled so no type is
    systematically starved.
    For types without issue_label, always consider them available.
    Returns None if no draining type has work.
    """
    items = list(draining.items())
    random.shuffle(items)
    for name, wt in items:
        issue_label = wt.get("issue_label", "")
        if issue_label:
            try:
                r = coordination(config, "queue-depth", issue_label)
                depth = int(r.stdout.strip())
            except (ValueError, subprocess.TimeoutExpired):
                depth = 0
            if depth > 0:
                return name
        else:
            # No label filter — treat as always having work
            return name
    return None


def dispatch_queue_balance(config: dict, queue_depth: int,
                           worker_types: dict,
                           state: AgentState | None = None) -> str | None:
    """Low queue → locked types (queue-filling), high queue → unlocked types (queue-draining)."""
    min_queue = cfg_get(config, "dispatch", "min_queue", default=3)

    # Separate types into queue-filling (have locks) and queue-draining (no locks)
    filling = {k: v for k, v in worker_types.items() if v.get("lock")}
    draining = {k: v for k, v in worker_types.items() if not v.get("lock")}

    if queue_depth < min_queue and filling:
        # Try to acquire lock for a filling type
        for name, wt in filling.items():
            lock_name = wt["lock"]
            r = coordination(config, f"lock-{lock_name}")
            if r.returncode == 0:
                if state:
                    state.lock_held = lock_name
                    state.write()
                return name
        # Lock held — fall back to draining if queue > 0
        if queue_depth > 0 and draining:
            return _choose_draining(config, draining)
        # Queue empty and lock held — wait
        return None
    elif draining:
        chosen = _choose_draining(config, draining)
        if chosen is not None:
            return chosen
        # No labeled work available despite nonzero global queue (e.g. unlabeled issues
        # from before the typed-worker migration). Fall back to planner to create
        # properly-typed issues rather than stalling indefinitely.
        if filling:
            for name, wt in filling.items():
                lock_name = wt["lock"]
                r = coordination(config, f"lock-{lock_name}")
                if r.returncode == 0:
                    if state:
                        state.lock_held = lock_name
                        state.write()
                    return name
        return None
    elif filling:
        # Only filling types exist — try them
        for name, wt in filling.items():
            lock_name = wt["lock"]
            r = coordination(config, f"lock-{lock_name}")
            if r.returncode == 0:
                if state:
                    state.lock_held = lock_name
                    state.write()
                return name
        return None
    return None


_round_robin_idx = 0


def dispatch_round_robin(config: dict, queue_depth: int,
                          worker_types: dict,
                          state: AgentState | None = None) -> str | None:
    """Cycle through worker types, skipping locked ones."""
    global _round_robin_idx
    names = list(worker_types.keys())
    if not names:
        return None
    for _ in range(len(names)):
        name = names[_round_robin_idx % len(names)]
        _round_robin_idx += 1
        wt = worker_types[name]
        lock_name = wt.get("lock")
        if lock_name:
            r = coordination(config, f"lock-{lock_name}")
            if r.returncode != 0:
                continue
            if state:
                state.lock_held = lock_name
                state.write()
        return name
    return None


def dispatch_custom(config: dict, queue_depth: int,
                     worker_types: dict,
                     state: AgentState | None = None) -> str | None:
    """Run a custom dispatch script."""
    strategy = cfg_get(config, "dispatch", "strategy", default="")
    script = os.path.expanduser(strategy)
    env = dict(os.environ)
    env["POD_QUEUE_DEPTH"] = str(queue_depth)
    env["POD_AGENT_COUNT"] = str(len(read_all_agents()))
    env["POD_WORKER_TYPES"] = ",".join(worker_types.keys())
    try:
        r = subprocess.run(
            [script], capture_output=True, text=True, timeout=30, env=env,
            cwd=str(PROJECT_DIR),
        )
        if r.returncode != 0:
            return None
        name = r.stdout.strip()
        if name in worker_types:
            # If the chosen type has a lock, try to acquire it
            lock_name = worker_types[name].get("lock")
            if lock_name:
                lr = coordination(config, f"lock-{lock_name}")
                if lr.returncode != 0:
                    return None
                if state:
                    state.lock_held = lock_name
                    state.write()
            return name
        return None
    except (subprocess.TimeoutExpired, FileNotFoundError, OSError):
        return None


def dispatch(config: dict, state: AgentState | None = None) -> str | None:
    """Choose a worker type to run. Returns type name or None (wait).
    If state is provided, sets state.lock_held immediately upon lock acquisition."""
    strategy = cfg_get(config, "dispatch", "strategy", default="queue_balance")
    worker_types = cfg_get(config, "worker_types", default={})
    if not worker_types:
        say("No worker types configured")
        return None

    queue_depth = get_queue_depth(config)

    if strategy == "queue_balance":
        return dispatch_queue_balance(config, queue_depth, worker_types, state)
    elif strategy == "round_robin":
        return dispatch_round_robin(config, queue_depth, worker_types, state)
    elif os.path.exists(os.path.expanduser(strategy)):
        return dispatch_custom(config, queue_depth, worker_types, state)
    else:
        say(f"Unknown dispatch strategy: {strategy}, falling back to queue_balance")
        return dispatch_queue_balance(config, queue_depth, worker_types, state)


# ---------------------------------------------------------------------------
# Dead claim recovery
# ---------------------------------------------------------------------------

def _get_base_branch() -> str:
    """Auto-detect the default branch (e.g. 'main' or 'master')."""
    try:
        r = subprocess.run(
            ["gh", "repo", "view", "--json", "defaultBranchRef", "-q",
             ".defaultBranchRef.name"],
            capture_output=True, text=True, timeout=15, cwd=str(PROJECT_DIR),
        )
        if r.returncode == 0 and r.stdout.strip():
            return r.stdout.strip()
    except Exception:
        pass
    return "master"


def _get_repo() -> str:
    """Auto-detect GitHub repo (owner/name) from the current git remote."""
    try:
        r = subprocess.run(
            ["gh", "repo", "view", "--json", "nameWithOwner", "-q", ".nameWithOwner"],
            capture_output=True, text=True, timeout=15, cwd=str(PROJECT_DIR),
        )
        if r.returncode == 0 and r.stdout.strip():
            return r.stdout.strip()
    except Exception:
        pass
    # Fallback: parse git remote
    try:
        r = subprocess.run(
            ["git", "remote", "get-url", "origin"],
            capture_output=True, text=True, timeout=10, cwd=str(PROJECT_DIR),
        )
        url = r.stdout.strip()
        # Handle SSH (git@github.com:owner/repo.git) and HTTPS
        import re as _re
        m = _re.search(r"github\.com[:/](.+?)(?:\.git)?$", url)
        if m:
            return m.group(1)
    except Exception:
        pass
    return "unknown/unknown"


def _release_claim(issue_str: str, session_uuid: str, restart_count: int) -> bool:
    """Remove the 'claimed' label from an issue and leave an explanatory comment.
    Returns True on success, False on failure (caller may revert history deletion)."""
    repo = _get_repo()
    try:
        r1 = subprocess.run(
            ["gh", "issue", "edit", issue_str, "--repo", repo, "--remove-label", "claimed"],
            capture_output=True, timeout=30, cwd=str(PROJECT_DIR),
        )
        if r1.returncode != 0:
            log(f"Failed to remove claimed label on #{issue_str}: {r1.stderr.decode().strip()}")
            return False
        msg = (f"Claim released — worker session `{session_uuid}` died after "
               f"{restart_count} restart attempt(s). Available for reclaim.")
        r2 = subprocess.run(
            ["gh", "issue", "comment", issue_str, "--repo", repo, "--body", msg],
            capture_output=True, timeout=30, cwd=str(PROJECT_DIR),
        )
        if r2.returncode != 0:
            log(f"Failed to comment on #{issue_str}: {r2.stderr.decode().strip()}")
            return False
        log(f"Released claim on #{issue_str}")
        return True
    except Exception as e:
        log(f"Failed to release claim on #{issue_str}: {e}")
        return False


def sync_claims_from_github():
    """On pod startup, rebuild claim-history.json from GitHub for any claimed
    issues we have no local record of. This lets pod reattach to sessions
    that were running before a pod restart."""
    import re as _re
    repo = _get_repo()
    cwd = str(PROJECT_DIR)

    try:
        r = subprocess.run(
            ["gh", "issue", "list", "--label", "agent-plan", "--label", "claimed",
             "--state", "open", "--limit", "20", "--json", "number"],
            capture_output=True, text=True, timeout=30, cwd=cwd,
        )
        if r.returncode != 0:
            return
        issues = [iss["number"] for iss in json.loads(r.stdout)]
    except (subprocess.TimeoutExpired, OSError, json.JSONDecodeError):
        return

    if not issues:
        return

    with _claim_history_filelock():
        history = load_claim_history()
        changed = False
        for issue_num in issues:
            key = str(issue_num)
            if key in history:
                continue  # Already tracked locally
            # Fetch comments to find the most recent claim comment
            try:
                r = subprocess.run(
                    ["gh", "api", f"repos/{repo}/issues/{issue_num}/comments",
                     "--jq", '[.[] | select(.body | startswith("Claimed by session")) | .body] | last'],
                    capture_output=True, text=True, timeout=30, cwd=cwd,
                )
                if r.returncode != 0:
                    continue
                comment_body = r.stdout.strip().strip('"')
            except (subprocess.TimeoutExpired, OSError):
                continue
            # Parse: "Claimed by session `UUID` on branch `agent/SHORT_ID`"
            m = _re.search(r'Claimed by session `([^`]+)` on branch `agent/([^`]+)`', comment_body)
            if not m:
                continue
            session_uuid, short_id = m.group(1), m.group(2)
            history[key] = {"session_uuid": session_uuid, "short_id": short_id, "restart_count": 0}
            changed = True
            log(f"Recovered claim: issue #{issue_num} → session {session_uuid[:8]} (short: {short_id})")
        if changed:
            _save_claim_history(history)


def check_dead_claimed_issues(config: dict):
    """Detect locally-known sessions that claimed issues but are now dead.
    Restart them up to max_claim_restarts times, then release the claim.

    Collects actions under the file lock, then executes them outside to avoid
    fork-under-lock (spawn_agent forks) and subprocess-under-lock (_release_claim).
    """
    history = load_claim_history()
    if not history:
        return

    agents = read_all_agents()
    live_uuids = {a.uuid for a in agents if a.status not in ("dead", "stopped")}
    max_restarts = cfg_get(config, "monitor", "max_claim_restarts", default=1)

    to_restart: list[tuple[str, str, str, int]] = []   # (short_id, session_uuid, issue_str, new_count)
    to_release: list[tuple[str, str, int]] = []         # (issue_str, session_uuid, restart_count)

    with _claim_history_filelock():
        history = load_claim_history()  # Re-read under lock
        changed = False
        for issue_str, info in list(history.items()):
            session_uuid = info.get("session_uuid", "")
            if not session_uuid or session_uuid in live_uuids:
                continue  # Still running or malformed entry

            restart_count = info.get("restart_count", 0)
            short_id = info.get("short_id", "")

            if restart_count < max_restarts:
                to_restart.append((short_id, session_uuid, issue_str, restart_count + 1))
                info["restart_count"] = restart_count + 1
                changed = True
            else:
                to_release.append((issue_str, session_uuid, restart_count))
                del history[issue_str]
                changed = True

        if changed:
            _save_claim_history(history)
    # Lock released before any fork or subprocess call

    for short_id, session_uuid, issue_str, new_count in to_restart:
        log(f"Dead session {session_uuid} claimed #{issue_str}, "
            f"restarting (attempt {new_count}/{max_restarts})")
        spawn_agent(config, agent_id=short_id, resume_uuid=session_uuid)

    failed_releases: list[tuple[str, str, int]] = []
    for issue_str, session_uuid, restart_count in to_release:
        log(f"Max restarts reached for #{issue_str}, releasing claim")
        if not _release_claim(issue_str, session_uuid, restart_count):
            failed_releases.append((issue_str, session_uuid, restart_count))

    # Re-add any failed releases so we retry next housekeeping cycle
    if failed_releases:
        with _claim_history_filelock():
            history = load_claim_history()
            for issue_str, session_uuid, restart_count in failed_releases:
                if issue_str not in history:
                    history[issue_str] = {
                        "session_uuid": session_uuid,
                        "short_id": "",
                        "restart_count": restart_count,
                    }
            _save_claim_history(history)


# ---------------------------------------------------------------------------
# Agent Lifecycle
# ---------------------------------------------------------------------------

def setup_worktree(config: dict, short_id: str) -> tuple[str, str]:
    """Create a fresh git worktree. Returns (worktree_path, branch_name)."""
    base = PROJECT_DIR / cfg_get(config, "project", "worktree_base", default="worktrees")
    base.mkdir(parents=True, exist_ok=True)
    wt_dir = base / short_id
    branch = f"agent/{short_id}"

    # Fetch latest default branch
    base_branch = _get_base_branch()
    subprocess.run(
        ["git", "-C", str(PROJECT_DIR), "fetch", "origin", base_branch, "--quiet"],
        capture_output=True, timeout=60,
    )

    # Clean up any leftover worktree/branch from a crash
    if wt_dir.exists():
        subprocess.run(
            ["git", "-C", str(PROJECT_DIR), "worktree", "remove", "--force", str(wt_dir)],
            capture_output=True, timeout=30,
        )
    subprocess.run(
        ["git", "branch", "-D", branch],
        capture_output=True, timeout=10, cwd=str(PROJECT_DIR),
    )

    # Create worktree
    r = subprocess.run(
        ["git", "-C", str(PROJECT_DIR), "worktree", "add", "-b", branch,
         str(wt_dir), f"origin/{base_branch}", "--quiet"],
        capture_output=True, timeout=60, check=True,
    )
    if not wt_dir.exists():
        raise subprocess.CalledProcessError(
            0, "git worktree add",
            output=r.stdout, stderr=(r.stderr + b" [dir not created]"),
        )

    return str(wt_dir), branch


def copy_build_cache(wt_dir: str, config: dict):
    """rsync build cache directory into worktree for faster builds."""
    cache_dir = cfg_get(config, "project", "build_cache_dir", default=".lake")
    cache_src = PROJECT_DIR / cache_dir
    if cache_src.is_dir():
        subprocess.run(
            ["rsync", "-a", "--quiet", f"{cache_src}/", f"{wt_dir}/{cache_dir}/"],
            capture_output=True, timeout=120,
        )


def cleanup_worktree(wt_dir: str, branch: str):
    """Remove worktree and delete branch."""
    if os.path.isdir(wt_dir):
        subprocess.run(
            ["git", "-C", str(PROJECT_DIR), "worktree", "remove", "--force", wt_dir],
            capture_output=True, timeout=30,
        )
    subprocess.run(
        ["git", "branch", "-D", branch],
        capture_output=True, timeout=10, cwd=str(PROJECT_DIR),
    )


def launch_claude(config: dict, session_uuid: str, prompt: str,
                   wt_dir: str,
                   claude_config_dir: Path | None = None) -> subprocess.Popen:
    """Launch claude in the worktree directory."""
    model = cfg_get(config, "claude", "model", default="opus")
    session_dir = PROJECT_DIR / cfg_get(config, "project", "session_dir", default="sessions")
    session_dir.mkdir(parents=True, exist_ok=True)
    stdout_path = session_dir / f"{session_uuid}.stdout"

    # Determine JSONL dir for this worktree
    jsonl_dir = _claude_projects_dir(claude_config_dir) / wt_dir.replace("/", "-")

    # Check if we can resume an existing session
    local_jsonl = jsonl_dir / f"{session_uuid}.jsonl"
    claude_args = ["claude", "--model", model]
    if local_jsonl.exists():
        claude_args += ["--resume", session_uuid]
    else:
        claude_args += ["--session-id", session_uuid]
    if claude_config_dir is not None:
        claude_args += ["--dangerously-skip-permissions"]
    claude_args += ["-p", prompt]

    env = dict(os.environ)
    env["ANTHROPIC_API_KEY"] = ""  # Force subscription auth
    env["POD_SESSION_ID"] = session_uuid
    env["POD_IS_RESUME"] = "1" if local_jsonl.exists() else "0"
    # Inject bundled data dir into PATH so agents find `coordination`
    env["PATH"] = str(_data_dir()) + os.pathsep + env.get("PATH", "")
    if claude_config_dir is not None:
        env["CLAUDE_CONFIG_DIR"] = str(claude_config_dir)

    stdout_fd = os.open(str(stdout_path), os.O_WRONLY | os.O_CREAT | os.O_TRUNC, 0o644)
    proc = subprocess.Popen(
        claude_args,
        stdout=stdout_fd,
        stderr=subprocess.STDOUT,
        cwd=wt_dir,
        env=env,
        start_new_session=True,  # Create process group for clean killing
    )
    os.close(stdout_fd)  # Child inherited it; parent no longer needs it
    return proc


def get_jsonl_path(wt_dir: str, session_uuid: str,
                   claude_config_dir: Path | None = None) -> str:
    """Compute JSONL file path for a session."""
    jsonl_dir = _claude_projects_dir(claude_config_dir) / wt_dir.replace("/", "-")
    return str(jsonl_dir / f"{session_uuid}.jsonl")


# ---------------------------------------------------------------------------
# Agent Process (forked background process)
# ---------------------------------------------------------------------------

# Globals for signal handlers
_agent_state: AgentState | None = None
_claude_proc: subprocess.Popen | None = None
_agent_config: dict = {}


def _sigterm_handler(signum, frame):
    """Handle SIGTERM: kill claude, unclaim, cleanup, exit."""
    global _agent_state, _claude_proc, _agent_config
    state = _agent_state
    config = _agent_config

    if state:
        log(f"Agent {state.short_id} received SIGTERM")
        state.status = "killed"
        state.write()

    # Kill claude subprocess
    if _claude_proc and _claude_proc.poll() is None:
        try:
            os.killpg(os.getpgid(_claude_proc.pid), signal.SIGTERM)
        except (ProcessLookupError, OSError):
            pass
        try:
            _claude_proc.wait(timeout=5)
        except subprocess.TimeoutExpired:
            try:
                os.killpg(os.getpgid(_claude_proc.pid), signal.SIGKILL)
            except (ProcessLookupError, OSError):
                pass

    if state and config:
        # Unclaim issue if claimed and no PR yet
        if state.claimed_issue > 0 and state.pr_number == 0:
            try:
                coordination(
                    config, "skip", str(state.claimed_issue),
                    f"Agent killed by operator (session {state.uuid})",
                    env_extra={"POD_SESSION_ID": state.uuid},
                )
                clear_claim(state.claimed_issue)
                log(f"Unclaimed issue #{state.claimed_issue}")
            except Exception:
                pass

        # Release lock if held
        if state.lock_held:
            try:
                coordination(config, f"unlock-{state.lock_held}")
                log(f"Released {state.lock_held} lock")
            except Exception:
                pass

        # Cleanup worktree
        if state.worktree and state.branch:
            cleanup_worktree(state.worktree, state.branch)

        state.remove_file()

    os._exit(0)


def _sigusr1_handler(signum, frame):
    """Handle SIGUSR1: set finishing flag."""
    global _agent_state
    if _agent_state:
        _agent_state.finishing = True
        _agent_state.status = "finishing"
        _agent_state.write()
        log(f"Agent {_agent_state.short_id} marked as finishing")


def agent_process_main(config: dict, agent_id: str | None = None,
                        resume_uuid: str | None = None):
    """Entry point for a forked agent process. Runs the agent loop."""
    global _agent_state, _claude_proc, _agent_config
    _agent_config = config

    short_id = agent_id or uuid.uuid4().hex[:8]

    my_pid = os.getpid()
    state = AgentState(
        short_id=short_id,
        pid=my_pid,
        pid_start_time=_get_pid_start_time(my_pid),
        status="starting",
        resume_session_uuid=resume_uuid or "",
    )
    _agent_state = state
    state.write()

    # Install signal handlers
    signal.signal(signal.SIGTERM, _sigterm_handler)
    signal.signal(signal.SIGUSR1, _sigusr1_handler)

    poll_interval = cfg_get(config, "monitor", "poll_interval", default=2)
    quota_retry = cfg_get(config, "claude", "quota_retry_seconds", default=60)
    worker_types = cfg_get(config, "worker_types", default={})
    pricing = cfg_get(config, "pricing", default={})

    claude_config_dir = ensure_claude_config(config)
    if claude_config_dir:
        log(f"Agent {short_id}: using isolated CLAUDE_CONFIG_DIR={claude_config_dir}")

    log(f"Agent {short_id} started (PID {os.getpid()})")

    iteration = 0
    while not state.finishing:
        iteration += 1
        state.loop_iteration = iteration

        # --- Quota check ---
        state.status = "waiting_quota"
        state.write()
        while not check_quota(config, force=state.force_quota):
            if state.finishing:
                break
            # Re-read state file to pick up force_quota toggled by TUI
            try:
                sf = AGENTS_DIR / f"{short_id}.json"
                d = json.loads(sf.read_text())
                state.force_quota = d.get("force_quota", False)
            except (OSError, json.JSONDecodeError):
                pass
            if state.force_quota:
                log(f"Agent {short_id}: force_quota enabled, skipping wait")
                break
            log(f"Agent {short_id}: no quota, sleeping {quota_retry}s")
            time.sleep(quota_retry)
        if state.finishing:
            break

        # --- Housekeeping ---
        try:
            coordination(config, "check-blocked")
        except Exception:
            pass
        try:
            check_dead_claimed_issues(config)
        except Exception:
            pass

        # --- Dispatch (sets state.lock_held atomically if lock acquired) ---
        # If this agent was spawned to resume a specific session, skip dispatch.
        _resume_uuid = state.resume_session_uuid
        if _resume_uuid:
            state.resume_session_uuid = ""
            chosen_type = "work"
            prompt = "You were interrupted mid-task. Review your conversation history and continue where you left off."
            lock_name = ""
            wt_config = {}
            state.worker_type = chosen_type
            state.write()
            log(f"Agent {short_id}: resuming session {_resume_uuid}")
        else:
            _resume_uuid = None
            state.status = "dispatching"
            state.write()
            chosen_type = dispatch(config, state)
            if chosen_type is None:
                log(f"Agent {short_id}: dispatch returned None (waiting)")
                state.status = "waiting_dispatch"
                state.write()
                time.sleep(quota_retry)
                continue

            wt_config = worker_types.get(chosen_type, {})
            prompt = wt_config.get("prompt", f"/{chosen_type}")
            lock_name = wt_config.get("lock", "")

            state.worker_type = chosen_type
            state.write()

            log(f"Agent {short_id}: dispatched as {chosen_type}")

        # --- Session setup ---
        session_uuid = _resume_uuid if _resume_uuid else str(uuid.uuid4())
        state.uuid = session_uuid
        state.session_start = time.time()
        state.claimed_issue = 0
        state.pr_number = 0
        state.tokens_in = 0
        state.tokens_out = 0
        state.cache_read = 0
        state.cache_create = 0
        state.last_text = ""
        state.last_activity = 0.0

        try:
            wt_dir, branch = setup_worktree(config, short_id)
        except subprocess.CalledProcessError as e:
            log(f"Agent {short_id}: worktree setup failed: {e}")
            if lock_name:
                coordination(config, f"unlock-{lock_name}")
                state.lock_held = ""
            state.status = "error"
            state.write()
            time.sleep(10)
            continue

        if not os.path.isdir(wt_dir):
            log(f"Agent {short_id}: worktree dir missing after setup_worktree returned: {wt_dir}")
            if lock_name:
                coordination(config, f"unlock-{lock_name}")
                state.lock_held = ""
            state.status = "error"
            state.write()
            time.sleep(10)
            continue

        state.worktree = wt_dir
        state.branch = branch
        state.git_start = _git_rev(wt_dir)

        if wt_config.get("copy_build_cache", wt_config.get("copy_lake_cache", False)):
            copy_build_cache(wt_dir, config)

        # --- Start JSONL monitor ---
        jsonl_path = get_jsonl_path(wt_dir, session_uuid, claude_config_dir)
        stop_monitor = threading.Event()
        monitor_thread = threading.Thread(
            target=jsonl_monitor, args=(jsonl_path, state, stop_monitor),
            daemon=True,
        )
        monitor_thread.start()

        # --- Launch claude ---
        state.status = "running"
        state.write()
        log(f"Agent {short_id}: launching claude session {session_uuid} in {wt_dir}")

        try:
            _claude_proc = launch_claude(config, session_uuid, prompt, wt_dir,
                                        claude_config_dir)
        except (OSError, FileNotFoundError) as e:
            log(f"Agent {short_id}: failed to launch claude: {e}")
            stop_monitor.set()
            cleanup_worktree(wt_dir, branch)
            if lock_name:
                coordination(config, f"unlock-{lock_name}")
                state.lock_held = ""
            state.status = "error"
            state.write()
            time.sleep(10)
            continue

        # --- Monitor until claude exits ---
        _last_tracked_issue = 0
        while _claude_proc.poll() is None:
            state.write()
            # Track claim changes: write to history when claimed, clear when PR created
            if state.claimed_issue > 0 and state.pr_number == 0:
                if state.claimed_issue != _last_tracked_issue:
                    record_claim(state.claimed_issue, state.uuid, state.short_id)
                    _last_tracked_issue = state.claimed_issue
            elif state.pr_number > 0 and state.claimed_issue > 0:
                clear_claim(state.claimed_issue)
                _last_tracked_issue = 0
            time.sleep(poll_interval)

        exit_code = _claude_proc.returncode
        _claude_proc = None

        # --- Session ended ---
        stop_monitor.set()
        monitor_thread.join(timeout=5)

        elapsed = time.time() - state.session_start
        git_end = _git_rev(wt_dir)
        tok = token_summary(state, pricing)
        log(f"Agent {short_id}: session finished exit={exit_code} "
            f"duration={human_duration(elapsed)} {tok} "
            f"git:{state.git_start}..{git_end}")

        # --- Circuit breaker: sessions that exit too quickly are broken ---
        if elapsed < 15 and state.tokens_in == 0 and state.tokens_out == 0:
            rapid_failures = getattr(state, '_rapid_failures', 0) + 1
            state._rapid_failures = rapid_failures
            backoff = min(60 * rapid_failures, 300)
            log(f"Agent {short_id}: session exited in {elapsed:.0f}s with 0 tokens "
                f"(rapid failure #{rapid_failures}), backing off {backoff}s")
            cleanup_worktree(wt_dir, branch)
            state.worktree = ""
            state.branch = ""
            if lock_name:
                try:
                    coordination(config, f"unlock-{lock_name}")
                except Exception:
                    pass
                state.lock_held = ""
            state.status = "backoff"
            state.write()
            time.sleep(backoff)
            continue
        else:
            # Reset rapid failure counter on successful session
            state._rapid_failures = 0

        # --- Cleanup ---
        cleanup_worktree(wt_dir, branch)
        state.worktree = ""
        state.branch = ""

        if lock_name:
            try:
                coordination(config, f"unlock-{lock_name}")
            except Exception:
                pass
            state.lock_held = ""

        state.write()

    # --- Agent loop exited ---
    log(f"Agent {short_id} exiting (finishing={state.finishing})")
    state.status = "stopped"
    state.write()
    # Leave state file briefly so TUI can see "stopped", then remove
    time.sleep(2)
    state.remove_file()


def _git_rev(wt_dir: str) -> str:
    try:
        r = subprocess.run(
            ["git", "-C", wt_dir, "rev-parse", "--short", "HEAD"],
            capture_output=True, text=True, timeout=10,
        )
        return r.stdout.strip() or "unknown"
    except Exception:
        return "unknown"


def spawn_agent(config: dict, agent_id: str | None = None,
                resume_uuid: str | None = None) -> int:
    """Fork a new agent process. Returns PID of the intermediate child.

    Uses double-fork so the agent is orphaned (adopted by init) and never
    becomes a zombie — without touching SIGCHLD in the calling process.
    Corrupting SIGCHLD in an agent process that calls spawn_agent (via
    check_dead_claimed_issues) would break git's internal waitpid and cause
    silent failures in git worktree add.
    """
    pid = os.fork()
    if pid > 0:
        # Parent: wait for the short-lived intermediate child (exits immediately).
        # Retry on EINTR so a signal doesn't cause a spurious launch failure.
        while True:
            try:
                os.waitpid(pid, 0)
                break
            except ChildProcessError:
                break
            except InterruptedError:
                continue
        return pid
    # Intermediate child: fork the actual agent, then exit immediately so
    # the agent is adopted by init (no zombie, no SIGCHLD games needed).
    try:
        gc_pid = os.fork()
    except Exception:
        os._exit(1)
    if gc_pid > 0:
        os._exit(0)
    # Grandchild: the actual agent process, adopted by init upon exit.
    try:
        os.setsid()
        devnull_r = os.open(os.devnull, os.O_RDONLY)
        devnull_w = os.open(os.devnull, os.O_WRONLY)
        os.dup2(devnull_r, 0)
        os.dup2(devnull_w, 1)
        os.dup2(devnull_w, 2)
        os.close(devnull_r)
        os.close(devnull_w)
        agent_process_main(config, agent_id, resume_uuid)
    except Exception as e:
        log(f"Agent process crashed: {e}")
    finally:
        os._exit(0)


# ---------------------------------------------------------------------------
# TUI
# ---------------------------------------------------------------------------

def run_tui(config: dict):
    """Run the interactive curses TUI."""
    curses.wrapper(_tui_main, config)


def _tui_main(stdscr, config: dict):
    # Rebuild claim history from GitHub before starting, so pod can reattach
    # to sessions that were running before a pod restart.
    try:
        sync_claims_from_github()
    except Exception:
        pass

    curses.curs_set(0)
    stdscr.timeout(1000)  # 1 second refresh
    curses.start_color()
    curses.use_default_colors()
    curses.init_pair(1, curses.COLOR_GREEN, -1)    # Running / merged
    curses.init_pair(2, curses.COLOR_YELLOW, -1)    # Finishing / blocked
    curses.init_pair(3, curses.COLOR_RED, -1)        # Dead/error / failing
    curses.init_pair(4, curses.COLOR_CYAN, -1)       # Header / closed
    curses.init_pair(5, curses.COLOR_WHITE, curses.COLOR_BLUE)  # Selected
    curses.init_pair(6, curses.COLOR_MAGENTA, -1)    # PR open / has-pr

    selected_section = "agents"  # "agents" or "items"
    selected_idx = 0
    message = ""
    message_time = 0.0
    input_mode = ""  # "", "kill_confirm"
    cached_queue: int | None = None
    queue_fetch_time = 0.0
    cached_items: list[GHItem] = []
    displayed_items: list[GHItem] = []  # subset shown on screen (active-first order)
    items_fetch_time = 0.0
    cached_blocked_deps: dict[int, list[int]] = {}
    blocked_deps_fetch_time = 0.0
    CACHE_SECS = 30  # Refresh interval for GitHub API data

    pricing = cfg_get(config, "pricing", default={})

    # Compute all-time historical cost once at startup
    claude_config_dir = get_claude_config_dir(config)
    historical_cost = compute_historical_cost(pricing, claude_config_dir)
    # Track session-accumulated costs (persists across agent deaths)
    session_agent_costs: dict[str, float] = {}  # agent short_id -> last known cost
    # Accumulated cost from previous iterations (when token counters reset)
    session_cost_offsets: dict[str, float] = {}  # agent short_id -> sum of prior iterations' costs
    # Baseline costs for agents already running when pod started (to avoid double-counting)
    baseline_costs: dict[str, float] = {}
    for a in read_all_agents():
        if a.status not in ("stopped", "dead"):
            baseline_costs[a.short_id] = a.cost(pricing)

    while True:
        agents = read_all_agents()
        # Accumulate costs from all agents (including dying ones) before cleanup
        for a in agents:
            current = a.cost(pricing)
            prev = session_agent_costs.get(a.short_id)
            if prev is not None and current < prev:
                # Token counters were reset (new loop iteration) — accumulate previous cost
                session_cost_offsets[a.short_id] = session_cost_offsets.get(a.short_id, 0.0) + prev
            session_agent_costs[a.short_id] = current
        # Clean up dead agent state files
        for a in agents:
            if a.status == "dead":
                a.remove_file()
        agents = [a for a in agents if a.status != "dead"]

        height, width = stdscr.getmaxyx()
        stdscr.erase()

        # --- Header ---
        # session_cost = delta from agents already running at startup + full cost of new agents
        session_cost = sum(
            session_cost_offsets.get(sid, 0.0) + cost - baseline_costs.get(sid, 0.0)
            for sid, cost in session_agent_costs.items()
        )
        session_runs = sum(1 for sid in session_agent_costs if sid not in baseline_costs)
        running = sum(1 for a in agents if a.status not in ("stopped", "dead"))

        # Cache GitHub data to avoid API calls every second
        now = time.time()
        if cached_queue is None or now - queue_fetch_time > CACHE_SECS:
            try:
                cached_queue = get_queue_depth(config)
                queue_fetch_time = now
            except Exception:
                pass
        if now - items_fetch_time > CACHE_SECS:
            try:
                cached_items = fetch_issues_and_prs()
                items_fetch_time = now
            except Exception:
                pass
        if now - blocked_deps_fetch_time > CACHE_SECS:
            try:
                cached_blocked_deps = fetch_blocked_deps()
                blocked_deps_fetch_time = now
            except Exception:
                pass

        all_time = historical_cost + session_cost
        session_info = f"${session_cost:.2f} this session, {session_runs} run{'s' if session_runs != 1 else ''}"
        if cached_queue is not None:
            header = f" pod -- {running} agent{'s' if running != 1 else ''} running | queue: {cached_queue} | ${all_time:.2f} total ({session_info})"
        else:
            header = f" pod -- {running} agent{'s' if running != 1 else ''} running | ${all_time:.2f} total ({session_info})"

        _addstr(stdscr, 0, 0, header[:width], curses.color_pair(4) | curses.A_BOLD)
        _addstr(stdscr, 1, 0, "─" * min(width, 80), curses.color_pair(4))

        # --- Agent table header ---
        col_fmt = "  {:>2} {:8} {:16} {:>6} {:>15} {}"
        hdr = " " + col_fmt.format("#", "ID", "Type", "Time", "Tokens", "Activity")
        _addstr(stdscr, 2, 0, hdr[:width], curses.A_DIM)

        # --- Clamp agent selection (items clamped after panel renders) ---
        if selected_section == "agents":
            if not agents:
                selected_idx = 0
            else:
                selected_idx = min(selected_idx, len(agents) - 1)

        is_agent_selected = selected_section == "agents"

        # --- Agent rows ---
        agents_rendered = 0
        for i, agent in enumerate(agents):
            row = 3 + i
            if row >= height - 3:
                break
            agents_rendered = i + 1

            # Mode label
            mode = agent.worker_type or "???"
            if agent.claimed_issue > 0:
                if agent.pr_number > 0:
                    mode = f"{mode} #{agent.claimed_issue}->PR"
                else:
                    mode = f"{mode} #{agent.claimed_issue}"
            if agent.finishing:
                mode += " (fin)"
            if agent.force_quota:
                mode += " !"

            # Elapsed
            elapsed = ""
            if agent.session_start > 0:
                elapsed = human_duration(time.time() - agent.session_start)

            # Tokens
            tok = token_summary(agent, pricing)

            # Activity text
            activity = agent.last_text or agent.status
            # Thinking detection: if JSONL is stale but process is alive
            if (agent.last_activity > 0 and
                    time.time() - agent.last_activity > 10 and
                    agent.status == "running"):
                stale = int(time.time() - agent.last_activity)
                activity = f"thinking {human_duration(stale)}"

            # Sanitize: collapse newlines/tabs to spaces
            activity = " ".join(activity.split())

            # Truncate activity to fit (1 extra for marker char)
            prefix = col_fmt.format(i + 1, agent.short_id, mode[:16], elapsed, tok, "")
            max_act = width - len(prefix) - 1  # -1 for marker
            if max_act > 10 and len(activity) > max_act:
                activity = activity[:max_act - 3] + "..."

            marker = ">" if is_agent_selected and i == selected_idx else " "
            line = f"{marker}" + col_fmt.format(i + 1, agent.short_id, mode[:16], elapsed, tok, activity)

            attr = curses.A_NORMAL
            if is_agent_selected and i == selected_idx:
                attr = curses.color_pair(5) | curses.A_BOLD
            elif agent.status == "finishing" or agent.finishing:
                attr = curses.color_pair(2)
            elif agent.status in ("dead", "error", "killed"):
                attr = curses.color_pair(3)
            elif agent.status == "running":
                attr = curses.color_pair(1)

            _addstr(stdscr, row, 0, line[:width], attr)

        # --- Issues/PRs panel ---
        # Row budget: 3 top (header+sep+colhdr) + rendered agents + 3 bottom (sep+help+msg)
        agents_end = 3 + agents_rendered
        footer_fixed = 3  # separator + help + message
        avail_for_items = height - agents_end - footer_fixed
        # Need: 1 blank + 1 separator + 1 header + at least 1 item row = 4
        items_shown = 0
        items_start_row = 0
        displayed_items = []  # reset each frame
        if cached_items and avail_for_items >= 4:
            max_item_rows = avail_for_items - 3  # blank + separator + header
            # Open items are always shown. Closed/merged fill remaining slots
            # (newest first), and are the first to go when space is tight.
            active = [it for it in cached_items if it.state not in ("closed", "merged")]
            inactive = [it for it in cached_items if it.state in ("closed", "merged")]
            slots_for_inactive = max(0, max_item_rows - len(active))
            # inactive is already sorted newest-first; take from the front
            selected = active + inactive[:slots_for_inactive]
            # Restore original sort order across the combined set
            selected_nums = {id(it) for it in selected}
            items_to_show = [it for it in cached_items if id(it) in selected_nums]
            displayed_items = items_to_show
            items_shown = len(items_to_show)

            items_start_row = agents_end + 1  # skip 1 blank line
            _addstr(stdscr, items_start_row, 0, "─" * min(width, 80), curses.color_pair(4))
            item_fmt = "  {:<6} {:<10} {:<8}  {}"
            item_hdr = " " + item_fmt.format("#", "State", "When", "Title")
            _addstr(stdscr, items_start_row + 1, 0, item_hdr[:width], curses.A_DIM)

            for j, item in enumerate(items_to_show):
                irow = items_start_row + 2 + j
                if irow >= height - footer_fixed:
                    items_shown = j
                    break

                # Unified state: combines kind, labels, CI, and state
                if item.kind == "issue":
                    if item.state == "closed":
                        state_str, state_color = "closed", curses.color_pair(4)
                    elif "claimed" in item.labels:
                        state_str, state_color = "claimed", curses.color_pair(1)
                    elif "has-pr" in item.labels:
                        state_str, state_color = "has-pr", curses.color_pair(6)
                    elif "blocked" in item.labels:
                        state_str, state_color = "blocked", curses.color_pair(2)
                    elif "replan" in item.labels:
                        state_str, state_color = "replan", curses.color_pair(2)
                    else:
                        state_str, state_color = "open", curses.A_NORMAL
                else:  # pr
                    if item.state == "merged":
                        state_str, state_color = "merged", curses.color_pair(4)
                    elif item.state == "closed":
                        state_str, state_color = "closed", curses.color_pair(4)
                    elif item.ci_status == "fail":
                        state_str, state_color = "failing", curses.color_pair(3)
                    else:
                        state_str, state_color = "open", curses.color_pair(6)

                age = timeago(item.timestamp)
                kind_prefix = "PR" if item.kind == "pr" else "I"
                state_display = f"{kind_prefix} {state_str}"

                title = item.title
                if "blocked" in item.labels and item.number in cached_blocked_deps:
                    deps = cached_blocked_deps[item.number]
                    title = f"[Blocked on {', '.join(f'#{d}' for d in deps)}] {title}"
                # Truncate title to fit
                prefix_len = 31
                max_title = width - prefix_len - 1
                if max_title > 10 and len(title) > max_title:
                    title = title[:max_title - 3] + "..."

                is_item_sel = not is_agent_selected and j == selected_idx
                marker = ">" if is_item_sel else " "
                num_str = f"#{item.number}"
                line = marker + item_fmt.format(num_str, state_display, age, title)

                attr = state_color
                if is_item_sel:
                    attr = curses.color_pair(5) | curses.A_BOLD

                _addstr(stdscr, irow, 0, line[:width], attr)

        # --- Clamp items selection (now that items_shown is known) ---
        if selected_section == "items":
            if items_shown == 0:
                selected_section = "agents"
                selected_idx = max(0, agents_rendered - 1)
                is_agent_selected = True
            else:
                selected_idx = min(selected_idx, items_shown - 1)

        # --- Footer separator ---
        footer_row = max(agents_end, height - footer_fixed)
        if items_shown > 0:
            footer_row = max(items_start_row + 2 + items_shown, height - footer_fixed)
        if footer_row < height - 1:
            _addstr(stdscr, footer_row, 0, "─" * min(width, 80), curses.color_pair(4))

        # --- Footer ---
        footer_row2 = footer_row + 1
        if input_mode == "kill_confirm":
            if agents and is_agent_selected and 0 <= selected_idx < len(agents):
                footer_text = f" Kill agent {agents[selected_idx].short_id}? (y/n)"
            else:
                footer_text = " No agent selected"
                input_mode = ""
        else:
            footer_text = " [a]dd  [f]inish  [k]ill  [o]pen  [!]force  [q]uit  [Q]uit all  ↑↓/1-9"

        if footer_row2 < height:
            _addstr(stdscr, footer_row2, 0, footer_text[:width], curses.A_DIM)

        # --- Message line ---
        if message and time.time() - message_time < 3:
            msg_row = footer_row2 + 1
            if msg_row < height:
                _addstr(stdscr, msg_row, 0, f" {message}"[:width], curses.A_BOLD)

        stdscr.refresh()

        # --- Input ---
        try:
            ch = stdscr.getch()
        except curses.error:
            continue

        if ch == -1:
            continue

        if input_mode == "kill_confirm":
            if ch in (ord("y"), ord("Y")):
                if agents and is_agent_selected and 0 <= selected_idx < len(agents):
                    agent = agents[selected_idx]
                    _kill_agent(config, agent)
                    message = f"Killed agent {agent.short_id}"
                    message_time = time.time()
            else:
                message = "Kill cancelled"
                message_time = time.time()
            input_mode = ""
            continue

        # Normal mode
        if ch == ord("q"):
            break
        elif ch == ord("Q"):
            # Quit all: finish all agents, then wait
            for a in agents:
                if a.pid > 0 and a.status not in ("stopped", "dead") and _pid_is_valid(a.pid, a.pid_start_time):
                    try:
                        os.kill(a.pid, signal.SIGUSR1)
                    except (ProcessLookupError, OSError):
                        pass
            message = "Sent finish signal to all agents. Waiting..."
            message_time = time.time()
            stdscr.refresh()
            # Wait for all to stop (with timeout display)
            deadline = time.time() + 600  # 10 min max
            while time.time() < deadline:
                live = [a for a in read_all_agents() if a.status not in ("stopped", "dead")]
                if not live:
                    break
                time.sleep(2)
            break
        elif ch == ord("a") or ch == ord("A"):
            spawn_agent(config)
            message = "Launched 1 agent"
            message_time = time.time()
        elif ch == ord("f") or ch == ord("F"):
            if is_agent_selected and agents and 0 <= selected_idx < len(agents):
                agent = agents[selected_idx]
                if agent.pid > 0 and _pid_is_valid(agent.pid, agent.pid_start_time):
                    try:
                        os.kill(agent.pid, signal.SIGUSR1)
                        message = f"Finish signal sent to {agent.short_id}"
                    except (ProcessLookupError, OSError):
                        message = f"Agent {agent.short_id} not running"
                else:
                    message = f"Agent {agent.short_id} not running"
                message_time = time.time()
        elif ch == ord("k") or ch == ord("K"):
            if is_agent_selected and agents and 0 <= selected_idx < len(agents):
                input_mode = "kill_confirm"
        elif ch == ord("o") or ch == ord("O"):
            if is_agent_selected:
                # Open agent's claimed issue
                if agents and 0 <= selected_idx < len(agents):
                    agent = agents[selected_idx]
                    issue = agent.claimed_issue
                    if issue > 0:
                        try:
                            subprocess.Popen(
                                ["gh", "issue", "view", str(issue), "--web"],
                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,
                                cwd=str(PROJECT_DIR),
                            )
                            message = f"Opening issue #{issue}"
                        except OSError as e:
                            message = f"Failed to open issue: {e}"
                    else:
                        message = f"Agent {agent.short_id} has no claimed issue"
                    message_time = time.time()
            else:
                # Open selected issue/PR
                if displayed_items and 0 <= selected_idx < len(displayed_items):
                    item = displayed_items[selected_idx]
                    gh_cmd = "issue" if item.kind == "issue" else "pr"
                    try:
                        subprocess.Popen(
                            ["gh", gh_cmd, "view", str(item.number), "--web"],
                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,
                            cwd=str(PROJECT_DIR),
                        )
                        message = f"Opening {item.kind} #{item.number}"
                    except OSError as e:
                        message = f"Failed to open: {e}"
                    message_time = time.time()
        elif ch == ord("!"):
            if is_agent_selected and agents and 0 <= selected_idx < len(agents):
                agent = agents[selected_idx]
                # Toggle force_quota in the agent's state file
                try:
                    sf = AGENTS_DIR / f"{agent.short_id}.json"
                    d = json.loads(sf.read_text())
                    new_val = not d.get("force_quota", False)
                    d["force_quota"] = new_val
                    tmp = sf.with_suffix(".tmp")
                    tmp.write_text(json.dumps(d, indent=2) + "\n")
                    tmp.rename(sf)
                    label = "ON" if new_val else "OFF"
                    message = f"Force quota {label} for {agent.short_id}"
                except (OSError, json.JSONDecodeError) as e:
                    message = f"Failed to toggle force: {e}"
                message_time = time.time()
        elif ch == curses.KEY_UP:
            if selected_section == "agents":
                if selected_idx > 0:
                    selected_idx -= 1
                # At top of agents — stay (nowhere to go)
            else:
                if selected_idx > 0:
                    selected_idx -= 1
                elif agents:
                    # Jump to last agent
                    selected_section = "agents"
                    selected_idx = len(agents) - 1
        elif ch == curses.KEY_DOWN:
            if selected_section == "agents":
                if agents and selected_idx < len(agents) - 1:
                    selected_idx += 1
                elif items_shown > 0:
                    # Jump to first item
                    selected_section = "items"
                    selected_idx = 0
            else:
                if selected_idx < items_shown - 1:
                    selected_idx += 1
        elif 49 <= ch <= 57:  # 1-9
            idx = ch - 49
            if idx < len(agents):
                selected_section = "agents"
                selected_idx = idx


def _addstr(win, y: int, x: int, s: str, attr: int = 0):
    """Safe addstr that doesn't crash on edge writes."""
    try:
        win.addstr(y, x, s, attr)
    except curses.error:
        pass


def _kill_agent(config: dict, agent: AgentState):
    """Send SIGTERM to an agent process."""
    if agent.pid > 0 and _pid_is_valid(agent.pid, agent.pid_start_time):
        try:
            os.kill(agent.pid, signal.SIGTERM)
        except (ProcessLookupError, OSError):
            # Process already dead — clean up manually
            if agent.claimed_issue > 0 and agent.pr_number == 0:
                try:
                    coordination(
                        config, "skip", str(agent.claimed_issue),
                        f"Agent killed by operator (session {agent.uuid})",
                        env_extra={"POD_SESSION_ID": agent.uuid},
                    )
                except Exception:
                    pass
            if agent.lock_held:
                try:
                    coordination(config, f"unlock-{agent.lock_held}")
                except Exception:
                    pass
            if agent.worktree and agent.branch:
                cleanup_worktree(agent.worktree, agent.branch)
            agent.remove_file()


# ---------------------------------------------------------------------------
# CLI Commands
# ---------------------------------------------------------------------------

def cmd_list(config: dict, args):
    """Show running agents."""
    agents = read_all_agents()
    pricing = cfg_get(config, "pricing", default={})

    if not agents:
        print("No agents running.")
        return

    alive = [a for a in agents if a.status != "dead"]
    dead = [a for a in agents if a.status == "dead"]

    # Clean up dead state files
    for a in dead:
        a.remove_file()

    if not alive:
        print("No agents running.")
        return

    fmt = "{:>2}  {:8}  {:16}  {:>6}  {:>15}  {}"
    print(fmt.format("#", "ID", "Type", "Time", "Tokens", "Status"))
    print("─" * 74)
    for i, a in enumerate(alive):
        mode = a.worker_type or "???"
        if a.claimed_issue > 0:
            if a.pr_number > 0:
                mode = f"{mode} #{a.claimed_issue}->PR"
            else:
                mode = f"{mode} #{a.claimed_issue}"
        if a.finishing:
            mode += " (fin)"

        elapsed = human_duration(time.time() - a.session_start) if a.session_start > 0 else ""
        tok = token_summary(a, pricing)
        activity = a.last_text or a.status

        print(fmt.format(i + 1, a.short_id, mode[:16], elapsed, tok, activity[:40]))


def cmd_add(config: dict, args):
    """Launch new agents."""
    n = args.count if args.count else 1
    for _ in range(n):
        pid = spawn_agent(config)
        say(f"Launched agent (PID {pid})")
    print(f"Launched {n} agent{'s' if n != 1 else ''}.")


def cmd_finish(config: dict, args):
    """Signal agent(s) to finish after current work."""
    agents = read_all_agents()
    alive = [a for a in agents if a.status not in ("stopped", "dead")]

    if args.target == "all":
        targets = alive
    else:
        targets = [a for a in alive if a.short_id.startswith(args.target)]
        if not targets:
            print(f"No running agent matching '{args.target}'")
            return

    for a in targets:
        if not _pid_is_valid(a.pid, a.pid_start_time):
            print(f"Agent {a.short_id} not running (PID {a.pid})")
            continue
        try:
            os.kill(a.pid, signal.SIGUSR1)
            print(f"Finish signal sent to {a.short_id} (PID {a.pid})")
        except (ProcessLookupError, OSError):
            print(f"Agent {a.short_id} not running (PID {a.pid})")


def cmd_kill(config: dict, args):
    """Kill agent(s) immediately."""
    agents = read_all_agents()
    alive = [a for a in agents if a.status not in ("stopped", "dead")]

    if args.target == "all":
        targets = alive
    else:
        targets = [a for a in alive if a.short_id.startswith(args.target)]
        if not targets:
            print(f"No running agent matching '{args.target}'")
            return

    for a in targets:
        _kill_agent(config, a)
        print(f"Killed {a.short_id} (PID {a.pid})")


def cmd_status(config: dict, args):
    """Show aggregate status."""
    agents = read_all_agents()
    alive = [a for a in agents if a.status not in ("stopped", "dead")]
    pricing = cfg_get(config, "pricing", default={})

    session_cost = sum(a.cost(pricing) for a in alive)
    total_in = sum(a.tokens_in + a.cache_read + a.cache_create for a in alive)
    total_out = sum(a.tokens_out for a in alive)
    claude_config_dir = get_claude_config_dir(config)
    historical_cost = compute_historical_cost(pricing, claude_config_dir)

    try:
        queue = get_queue_depth(config)
        print(f"Queue depth:    {queue}")
    except Exception:
        print("Queue depth:    (unavailable)")

    print(f"Running agents: {len(alive)}")

    types = {}
    for a in alive:
        t = a.worker_type or "unknown"
        types[t] = types.get(t, 0) + 1
    if types:
        parts = [f"{v} {k}" for k, v in types.items()]
        print(f"  Breakdown:    {', '.join(parts)}")

    print(f"Total tokens:   {fmt_tokens(total_in)} in / {fmt_tokens(total_out)} out")
    print(f"Running cost:   ${session_cost:.2f}")
    print(f"All-time cost:  ${historical_cost:.2f}")


def cmd_log(config: dict, args):
    """Tail agent's session stdout."""
    agents = read_all_agents()
    if args.target:
        matches = [a for a in agents if a.short_id.startswith(args.target)]
    else:
        # Default to most recent agent
        matches = sorted(agents, key=lambda a: a.session_start, reverse=True)[:1]

    if not matches:
        print("No matching agent found.")
        return

    agent = matches[0]
    session_dir = PROJECT_DIR / cfg_get(config, "project", "session_dir", default="sessions")
    stdout_path = session_dir / f"{agent.uuid}.stdout"
    if not stdout_path.exists():
        print(f"No log file for agent {agent.short_id}")
        return

    # Print last 50 lines
    lines = stdout_path.read_text().splitlines()
    for line in lines[-50:]:
        print(line)


def cmd_config(config: dict, args):
    """Print current configuration."""
    if args.edit:
        editor = os.environ.get("EDITOR", "vi")
        os.execlp(editor, editor, str(CONFIG_PATH))
    else:
        print(CONFIG_PATH.read_text(), end="")


# ---------------------------------------------------------------------------
# Init / Update / Coordination subcommands
# ---------------------------------------------------------------------------


def _populate_claude_config():
    """Copy bundled claude-config/ into .pod/claude-config/."""
    src = _data_dir() / "claude-config"
    dst = ISOLATED_CONFIG_DIR
    if dst.exists():
        shutil.rmtree(dst)
    shutil.copytree(str(src), str(dst))


def cmd_init(args):
    """Bootstrap .pod/ in the current git repo."""
    # Verify we're in a git repo
    r = subprocess.run(["git", "rev-parse", "--show-toplevel"],
                       capture_output=True, text=True, timeout=5)
    if r.returncode != 0:
        print("Not in a git repository.", file=sys.stderr)
        sys.exit(1)
    git_root = Path(r.stdout.strip())
    pod_dir = git_root / ".pod"
    config_path = pod_dir / "config.toml"

    pod_dir.mkdir(parents=True, exist_ok=True)
    (pod_dir / "agents").mkdir(exist_ok=True)

    # .gitignore
    gitignore = pod_dir / ".gitignore"
    gitignore.write_text("agents/\npod.log\nclaim-history.*\nclaude-config/\n")

    # config.toml
    if not config_path.exists() or getattr(args, "force", False):
        config_path.write_text(DEFAULT_CONFIG)
        print(f"  wrote {config_path.relative_to(git_root)}")
    else:
        print(f"  {config_path.relative_to(git_root)} already exists (use --force to overwrite)")

    # claude-config from package data
    global ISOLATED_CONFIG_DIR
    ISOLATED_CONFIG_DIR = pod_dir / "claude-config"
    _populate_claude_config()
    print(f"  populated {ISOLATED_CONFIG_DIR.relative_to(git_root)}/")
    print("pod init complete.")


def cmd_update(args):
    """Re-populate .pod/claude-config/ from installed package."""
    if not POD_DIR.is_dir():
        print("No .pod/ directory found. Run 'pod init' first.", file=sys.stderr)
        sys.exit(1)
    _populate_claude_config()
    print("Updated .pod/claude-config/ from installed package.")


def cmd_coordination(args):
    """Pass-through to bundled coordination script."""
    script = str(_data_dir() / "coordination")
    env = dict(os.environ)
    # Pass protected-files from config if available
    if CONFIG_PATH.exists():
        with open(CONFIG_PATH, "rb") as f:
            config = tomllib.load(f)
        pf = cfg_get(config, "project", "protected_files", default=["PLAN.md"])
        if isinstance(pf, list):
            pf = ":".join(pf)
        env["POD_PROTECTED_FILES"] = pf
    result = subprocess.run(
        [script] + args.coordination_args,
        cwd=str(PROJECT_DIR), env=env,
    )
    sys.exit(result.returncode)


# ---------------------------------------------------------------------------
# Entry point
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        prog="pod",
        description="pod — multi-agent manager",
    )
    sub = parser.add_subparsers(dest="command")

    # Subcommands that don't require ensure_config()
    p_init = sub.add_parser("init", help="Bootstrap .pod/ in current git repo")
    p_init.add_argument("--force", action="store_true",
                         help="Overwrite existing config.toml")

    sub.add_parser("update", help="Re-populate .pod/claude-config/ from package")

    p_coord = sub.add_parser("coordination",
                              help="Run bundled coordination script")
    p_coord.add_argument("coordination_args", nargs=argparse.REMAINDER,
                          help="Arguments passed to coordination")

    # Subcommands that require an existing .pod/config.toml
    sub.add_parser("list", help="Show running agents")

    p_add = sub.add_parser("add", help="Launch new agents")
    p_add.add_argument("count", type=int, nargs="?", default=1,
                        help="Number of agents to launch (default: 1)")

    p_finish = sub.add_parser("finish", help="Signal agent to finish current work")
    p_finish.add_argument("target", help="Agent ID prefix or 'all'")

    p_kill = sub.add_parser("kill", help="Kill agent immediately")
    p_kill.add_argument("target", help="Agent ID prefix or 'all'")

    sub.add_parser("status", help="Show aggregate status")

    p_log = sub.add_parser("log", help="Tail agent session output")
    p_log.add_argument("target", nargs="?", default=None,
                        help="Agent ID prefix (default: most recent)")

    p_config = sub.add_parser("config", help="Show configuration")
    p_config.add_argument("--edit", action="store_true",
                           help="Open config in $EDITOR")

    args = parser.parse_args()

    # Handle subcommands that don't require ensure_config()
    if args.command == "init":
        cmd_init(args)
        return
    elif args.command == "update":
        cmd_update(args)
        return
    elif args.command == "coordination":
        cmd_coordination(args)
        return

    config = ensure_config()

    if args.command is None:
        # No subcommand → TUI
        run_tui(config)
    elif args.command == "list":
        cmd_list(config, args)
    elif args.command == "add":
        cmd_add(config, args)
    elif args.command == "finish":
        cmd_finish(config, args)
    elif args.command == "kill":
        cmd_kill(config, args)
    elif args.command == "status":
        cmd_status(config, args)
    elif args.command == "log":
        cmd_log(config, args)
    elif args.command == "config":
        cmd_config(config, args)


if __name__ == "__main__":
    main()
